{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oRhTab-3Isg"
      },
      "source": [
        "## Fine-tuning Mistral 7b with AutoTrain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhDioAdc3ML5"
      },
      "source": [
        "Setup Runtime\n",
        "For fine-tuning Llama, a GPU instance is essential. Follow the directions below:\n",
        "\n",
        "- Go to `Runtime` (located in the top menu bar).\n",
        "- Select `Change Runtime Type`.\n",
        "- Choose `T4 GPU` (or a comparable option)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJZt3QI73kWF"
      },
      "source": [
        "### Step 1: Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UgvqeBz_3XvO"
      },
      "outputs": [],
      "source": [
        "!pip install pandas autotrain-advanced -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwStofw4257S",
        "outputId": "bb1cabf1-6b6e-4f85-fad0-4c29129902b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-03-31 14:06:12\u001b[0m | \u001b[36mautotrain.cli.run_setup\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mInstalling latest xformers\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-03-31 14:06:13\u001b[0m | \u001b[36mautotrain.cli.run_setup\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSuccessfully installed latest xformers\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-03-31 14:06:13\u001b[0m | \u001b[36mautotrain.cli.run_setup\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mInstalling latest PyTorch\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-03-31 14:06:16\u001b[0m | \u001b[36mautotrain.cli.run_setup\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mSuccessfully installed latest PyTorch\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!autotrain setup --update-torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kpNugcys2Uh",
        "outputId": "fded8488-e8db-4f04-9596-b53526316288"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.0)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl (76.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "  Attempting uninstall: bitsandbytes\n",
            "    Found existing installation: bitsandbytes 0.45.0\n",
            "    Uninstalling bitsandbytes-0.45.0:\n",
            "      Successfully uninstalled bitsandbytes-0.45.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "autotrain-advanced 0.8.36 requires bitsandbytes==0.45.0; sys_platform == \"linux\", but you have bitsandbytes 0.45.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.45.4\n"
          ]
        }
      ],
      "source": [
        "!pip install triton\n",
        "!pip install --upgrade bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-zXccJMZEx2"
      },
      "source": [
        "## Step 2: Connect to HuggingFace for Model Upload\n",
        "\n",
        "### Logging to Hugging Face\n",
        "To make sure the model can be uploaded to be used for Inference, it's necessary to log in to the Hugging Face hub.\n",
        "\n",
        "### Getting a Hugging Face token\n",
        "Steps:\n",
        "\n",
        "1. Navigate to this URL: https://huggingface.co/settings/tokens\n",
        "2. Create a write `token` and copy it to your clipboard\n",
        "3. Run the code below and enter your `token`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "ca3f33ecb9c8427ab5470edccde011ab",
            "b73f3add66a545efba5167a856ae5787",
            "9cd0a8a8677a43c480f2de842069968e",
            "02f6c09fcf8643389fc2cf1b3f9c668f",
            "782834571ce74235b2c4948ff9a47b70",
            "0cef031df2d64362bdfd505559345ce1",
            "4c4ecd6406da42aa8c7ee7e4f6c73f50",
            "47b948b31f0343ab9c2f5b9e4ec1fa3b",
            "74275c5ff18a4bb0912e482c8e535ea7",
            "52a12ac6bd564e3cbc6f365151988631",
            "376ebd3722eb4145964fec20db32d5d3",
            "d2c284204e554cbeae77e5f9e47248d7",
            "527e3d95ea90450086c9ff366c9bdc6f",
            "e543526b1645407ba56f8d7d06a703ca",
            "4b7f2b4a2cee4d08a77f1e5b63be8692",
            "60ca62f461c744f9b5e8fb3993231e7c",
            "8929504f7ade4c8bb2a0921e5b816b2d",
            "0208623ecbe842b6aa95c6fcee2b5ae8",
            "bb1457ce23234e2f86f20318977040c8",
            "bd9ccb8d007e4659bce33beff90a9424"
          ]
        },
        "id": "VzMLmLP86Ub-",
        "outputId": "8d50f7b2-6125-4ace-cba0-8bf8f76d47e5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca3f33ecb9c8427ab5470edccde011ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY932JBNZmtA"
      },
      "source": [
        "## Step 3: Upload your dataset\n",
        "\n",
        "Add your data set to the root directory in the Colab under the name train.csv. The AutoTrain command will look for your data there under that name.\n",
        "\n",
        "#### Don't have a data set and want to try finetuning on an example data set?\n",
        "If you don't have a dataset you can run these commands below to get an example data set and save it to train.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "NUb-rkeoZzZ6",
        "outputId": "46f7cd85-92b8-473c-d6cf-8bd284d32a21"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 111,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 111,\n        \"samples\": [\n          \"system_instruction: You are a research assistant chatbot designed to help users with academic research inquiries. Your role is to provide concise summaries; relevant citations; and guidance on further research steps. Always include a disclaimer that your information is based on available sources and should be verified with primary literature. Use clear; neutral language and avoid promoting any non-verified opinions.\\n few_shot_examples: [ { \\\"input\\\": \\\"Can you summarize recent advances in renewable energy research?\\\"; \\\"internal_thought\\\": \\\"Extract key trends and summarize findings without introducing personal bias. Provide a disclaimer about verification.\\\"; \\\"output\\\": \\\"Response: Recent research in renewable energy highlights significant improvements in solar panel efficiency and wind turbine technology. Key studies from 2023 indicate a 15% increase in conversion rates. Please note; this summary is based on available sources and should be cross-verified with primary research articles.\\\" } ]\\n state_tracking: [ \\\"Step 1: Identify the research topic and key terms.\\\"; \\\"Step 2: Summarize findings from reliable sources.\\\"; \\\"Step 3: Include citations and a disclaimer for verification.\\\"; \\\"Step 4: Provide guidance on further reading if needed.\\\" ]\\n output_format: Structured response with a summary of findings; citations; and a clear disclaimer. Guardrails: No personal opinions; no unverified claims; and clear recommendation to verify with primary literature.\",\n          \"system_instruction: You are an ethics consultant specializing in complex moral reasoning. Your task is to analyze ethical dilemmas from multiple perspectives; identifying key ethical principles in conflict; evaluating potential courses of action; and providing a nuanced analysis that respects diverse viewpoints. \\n few_shot_examples: [ { input: \\\"Is it ethical to use AI-generated art commercially without disclosure?\\\"; internal_thought: \\\"This involves principles of transparency; authenticity; economic impact on human artists; and consumer expectations. Need to consider utilitarian; deontological; and virtue ethics perspectives.\\\"; output: \\\"From a utilitarian perspective; non-disclosure may maximize market efficiency but potentially harms human artists and misleads consumers. Deontologically; transparency is a moral duty regardless of consequences. A virtue ethics approach suggests developing the character trait of honesty in creative enterprises. A balanced approach might involve clear disclosure while recognizing AI as a tool rather than a creator.\\\" }; { input: \\\"Should medical resources be allocated based on quality-adjusted life years?\\\"; internal_thought: \\\"This involves tensions between utilitarian efficiency and egalitarian justice. Need to consider implications for vulnerable populations; disability rights; and fair allocation principles.\\\"; output: \\\"Utilitarian approaches favor maximizing overall health outcomes through quality-adjusted metrics; while egalitarian perspectives emphasize equal access regardless of predicted outcomes. Practical solutions might involve using QALY as one factor among many; establishing minimum thresholds for all patients; and ensuring procedural justice in allocation decisions. Cultural and societal values must inform these decisions rather than pure algorithmic approaches.\\\" } ] \\n state_tracking: [ \\\"Step 1: Identify the key ethical principles in tension.\\\"; \\\"Step 2: Analyze the dilemma from multiple ethical frameworks.\\\"; \\\"Step 3: Consider impacts on various stakeholders.\\\"; \\\"Step 4: Evaluate potential resolutions and their trade-offs.\\\"; \\\"Step 5: Provide a nuanced conclusion acknowledging complexity.\\\" ] \\n output_format: A structured ethical analysis examining multiple perspectives; key principles in conflict; and potential resolutions with their implications.\",\n          \"system_instruction: You are a certified financial planner specializing in retirement planning. Your task is to provide tailored retirement planning strategies based on the individual's age; income; risk tolerance; and retirement goals. Incorporate tax-efficient investment approaches and long-term financial sustainability. \\n few_shot_examples: [ { input: \\\"What retirement strategies should a 35-year-old with moderate risk tolerance consider?\\\"; internal_thought: \\\"At 35; there's still considerable time until retirement. A diversified portfolio with higher equity allocation is appropriate for moderate risk tolerance. Tax-advantaged accounts should be maximized.\\\"; output: \\\"For a 35-year-old; focus on maximizing contributions to tax-advantaged accounts (401(k); IRA) with a 70-30 equity-to-fixed income portfolio. Consider Roth conversions during lower income years and automate savings of 15-20% of income for retirement.\\\" }; { input: \\\"How should a 55-year-old adjust their retirement strategy?\\\"; internal_thought: \\\"At 55; retirement is approaching. The focus should shift to preservation while maintaining growth. Catch-up contributions and debt reduction are important.\\\"; output: \\\"At 55; make catch-up contributions to retirement accounts; gradually shift to a more conservative 60-40 portfolio; and develop a retirement income strategy. Consider long-term care insurance and prioritize debt reduction before retirement.\\\" } ] \\n state_tracking: [ \\\"Step 1: Identify the individual's age; income; and retirement timeline.\\\"; \\\"Step 2: Determine appropriate asset allocation based on risk tolerance and time horizon.\\\"; \\\"Step 3: Recommend specific retirement accounts and contribution strategies.\\\"; \\\"Step 4: Outline withdrawal strategies and tax considerations for retirement.\\\" ] \\n output_format: A comprehensive retirement plan with specific investment allocations; contribution recommendations; and timeline milestones.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rejected_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 111,\n        \"samples\": [\n          \"You are a research assistant chatbot for academic inquiries. Assist users in finding and summarizing research information.\",\n          \"Solve a complex ethical dilemma.\",\n          \"Explain strategies for retirement planning.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-6a892f09-63d4-4ecb-921a-fe669e4a56ab\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>rejected_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>system_instruction: System-instruction: You ar...</td>\n",
              "      <td>Suggest medications for a patient with high bl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>system_instruction: You are an endocrinology s...</td>\n",
              "      <td>Provide treatment options for type 2 diabetes.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>system_instruction: You are a pediatric immuni...</td>\n",
              "      <td>Explain vaccination protocols for children.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>system_instruction: You are a financial analys...</td>\n",
              "      <td>Analyze the stock market trends for the last 6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>system_instruction: You are a certified financ...</td>\n",
              "      <td>Explain strategies for retirement planning.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>system_instruction: You are a predictive analy...</td>\n",
              "      <td>You are a predictive analytics assistant for f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>system_instruction: You are a healthcare assis...</td>\n",
              "      <td>You are a healthcare assistant. Provide treatm...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>system_instruction: You are an emergency respo...</td>\n",
              "      <td>You are an emergency response coordinator. Ass...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>system_instruction: You are a robotics managem...</td>\n",
              "      <td>You are a robotics management assistant. Coord...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>system_instruction: You are a virtual assistan...</td>\n",
              "      <td>You are a virtual assistant. Manage scheduling...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>111 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a892f09-63d4-4ecb-921a-fe669e4a56ab')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6a892f09-63d4-4ecb-921a-fe669e4a56ab button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6a892f09-63d4-4ecb-921a-fe669e4a56ab');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-757db176-bd89-4e21-bb3a-8b6b9432b3f6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-757db176-bd89-4e21-bb3a-8b6b9432b3f6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-757db176-bd89-4e21-bb3a-8b6b9432b3f6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_9fcf95fd-5003-4bb7-b3f0-b1abce4aafa0\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_9fcf95fd-5003-4bb7-b3f0-b1abce4aafa0 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                  text  \\\n",
              "0    system_instruction: System-instruction: You ar...   \n",
              "1    system_instruction: You are an endocrinology s...   \n",
              "2    system_instruction: You are a pediatric immuni...   \n",
              "3    system_instruction: You are a financial analys...   \n",
              "4    system_instruction: You are a certified financ...   \n",
              "..                                                 ...   \n",
              "106  system_instruction: You are a predictive analy...   \n",
              "107  system_instruction: You are a healthcare assis...   \n",
              "108  system_instruction: You are an emergency respo...   \n",
              "109  system_instruction: You are a robotics managem...   \n",
              "110  system_instruction: You are a virtual assistan...   \n",
              "\n",
              "                                         rejected_text  \n",
              "0    Suggest medications for a patient with high bl...  \n",
              "1       Provide treatment options for type 2 diabetes.  \n",
              "2          Explain vaccination protocols for children.  \n",
              "3    Analyze the stock market trends for the last 6...  \n",
              "4          Explain strategies for retirement planning.  \n",
              "..                                                 ...  \n",
              "106  You are a predictive analytics assistant for f...  \n",
              "107  You are a healthcare assistant. Provide treatm...  \n",
              "108  You are an emergency response coordinator. Ass...  \n",
              "109  You are a robotics management assistant. Coord...  \n",
              "110  You are a virtual assistant. Manage scheduling...  \n",
              "\n",
              "[111 rows x 2 columns]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "3mr4WrwHZ0pv",
        "outputId": "0f794fc3-431c-44d3-d0cf-07b13707669b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'role: You are an experienced high school math educator specializing in algebra. Your task is to create a detailed lesson plan that introduces quadratic equations; explains their properties; methods of solving; and practical applications. \\n state_tracking: [\"Step 1: Identify the core concepts related to quadratic equations (standard form; vertex form; roots; discriminant).\"; \"Step 2: Define clear learning objectives and outcomes.\"; \"Step 3: Design engaging instructional activities and real-world examples.\"; \"Step 4: Include formative assessment methods to gauge understanding.\"; \"Step 5: Provide a summary and list supplementary reference materials.\"] \\n few_shot_examples: [{input: \"How do you introduce quadratic equations in simple terms?\"; internal_thought: \"Link the concept with everyday situations and visual aids.\"; output: \"Begin by explaining that quadratic equations have the form ax² + bx + c = 0 and illustrate their use in modeling real-world scenarios like projectile motion or calculating areas.\"}; {input: \"Design an activity for solving quadratics by factoring.\"; internal_thought: \"Create an interactive group activity focusing on factoring techniques.\"; output: \"Plan a collaborative exercise where students work in small groups to factor simple quadratic equations and then share their solutions; followed by a class discussion.\"}] \\n output_format: A structured lesson plan document with objectives; instructional activities; assessments; and additional resources.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['text'][15]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEFbHxoPaDE_"
      },
      "source": [
        "## Step 4: Overview of AutoTrain Command\n",
        "\n",
        "#### Short Overview of What the Command Flags Do\n",
        "\n",
        "- `!autotrain`: Runs a shell command directly (commonly used in Jupyter or Colab notebooks).\n",
        "- `llm`: Specifies that you’re working on a language model task.\n",
        "- `--train`: Initiates the training process.\n",
        "- `--project-name`: Sets the name of the project (and output directory).\n",
        "- `--model abhishek/llama-2-7b-hf-small-shards`: Specifies the base model hosted on Hugging Face (here, \"llama-2-7b-hf-small-shards\" under the \"abhishek\" namespace).\n",
        "- `--data_path .`: Points to the directory containing your dataset. With `.` AutoTrain will look for your dataset (e.g. `train.csv`) in the current directory.\n",
        "- `--peft`: Enables Parameter-Efficient Fine-Tuning (PEFT) using techniques like LoRA.\n",
        "- `--lora-r`, `--lora-alpha`, `--lora-dropout`: Configure LoRA parameters (e.g., rank, scaling factor, and dropout rate).\n",
        "- `--quantization int4`: Enables 4-bit quantization to reduce model size and speed up inference with minimal precision loss.\n",
        "- `--lr 2e-4`: Sets the learning rate for training to 0.0002.\n",
        "- `--batch-size 12`: Sets the training batch size to 12.\n",
        "- `--epochs 3`: Instructs AutoTrain to iterate over your dataset 3 times (3 training epochs).\n",
        "- `--mixed-precision bf16`: Uses BF16 mixed-precision training for improved performance and memory efficiency.\n",
        "- `--trainer reward`: Specifies that the Reward Trainer should be used (ideal for training reward models).\n",
        "- `--target-modules q_proj,v_proj`: Limits fine-tuning to specific target modules (in this case, the query and value projection layers).\n",
        "- `--push-to-hub`: Automatically uploads the fine-tuned model to the Hugging Face Hub after training.\n",
        "\n",
        "---\n",
        "\n",
        "### Steps Needed Before Running\n",
        "\n",
        "1. **Project Name:** After `--project-name`, replace `<project-name>` with the name you'd like to assign to your project.\n",
        "2. **Repository ID:** After `--repo_id`, replace `<username>/<repository>` with your Hugging Face username and the desired repository name. (The repository will be created automatically if it doesn't exist.)\n",
        "3. **Dataset Location:** Ensure that your dataset (e.g. `train.csv`) is in the root directory since `--data_path .` directs AutoTrain to look there.\n",
        "4. **LoRA Parameters:** Adjust the LoRA parameters if needed. Here, `--target-modules q_proj,v_proj` indicates which parts of the model to fine-tune.\n",
        "5. **Mixed Precision & Trainer:** Confirm you want to use BF16 mixed-precision training along with the Reward Trainer.\n",
        "6. **Authentication:** If required, include your Hugging Face credentials using `--username` and `--token`.\n",
        "\n",
        "---\n",
        "\n",
        "### Example Command\n",
        "\n",
        "```bash\n",
        "!autotrain llm --train \\\n",
        "    --project-name my-project-name \\\n",
        "    --model mistralai/Mistral-7B-Instruct-v0.3 \\\n",
        "    --data_path . \\\n",
        "    --peft \\\n",
        "    --lora-r 16 \\\n",
        "    --lora-alpha 32 \\\n",
        "    --lora-dropout 0.1 \\\n",
        "    --quantization int4 \\\n",
        "    --lr 2e-4 \\\n",
        "    --batch-size 12 \\\n",
        "    --epochs 3 \\\n",
        "    --mixed-precision bf16 \\\n",
        "    --trainer reward \\\n",
        "    --target-modules q_proj,v_proj \\\n",
        "    --push-to-hub \\\n",
        "    --username USERNAME \\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFS31VJsZ-pa",
        "outputId": "7379fc5b-45bc-4e68-85e1-7174c6ea2bb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-03-31 14:56:29\u001b[0m | \u001b[36mautotrain.cli.run_llm\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mRunning LLM\u001b[0m\n",
            "\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2025-03-31 14:56:29\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m286\u001b[0m - \u001b[33m\u001b[1mParameters supplied but not used: inference, func, version, train, config, deploy, backend\u001b[0m\n",
            "\rSaving the dataset (0/1 shards):   0% 0/111 [00:00<?, ? examples/s]\rSaving the dataset (1/1 shards): 100% 111/111 [00:00<00:00, 21451.77 examples/s]\rSaving the dataset (1/1 shards): 100% 111/111 [00:00<00:00, 20713.07 examples/s]\n",
            "\rSaving the dataset (0/1 shards):   0% 0/111 [00:00<?, ? examples/s]\rSaving the dataset (1/1 shards): 100% 111/111 [00:00<00:00, 21673.47 examples/s]\rSaving the dataset (1/1 shards): 100% 111/111 [00:00<00:00, 20962.08 examples/s]\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-03-31 14:56:29\u001b[0m | \u001b[36mautotrain.backends.local\u001b[0m:\u001b[36mcreate\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mStarting local training...\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-03-31 14:56:29\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m514\u001b[0m - \u001b[1m['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'bf16', '-m', 'autotrain.trainers.clm', '--training_config', 'mistral-prompttune/training_params.json']\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-03-31 14:56:29\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m515\u001b[0m - \u001b[1m{'model': 'mistralai/Mistral-7B-Instruct-v0.3', 'project_name': 'mistral-prompttune', 'data_path': 'mistral-prompttune/autotrain-data', 'train_split': 'train', 'valid_split': None, 'add_eos_token': True, 'block_size': 1024, 'model_max_length': 2048, 'padding': 'right', 'trainer': 'reward', 'use_flash_attention_2': False, 'log': 'none', 'disable_gradient_checkpointing': False, 'logging_steps': -1, 'eval_strategy': 'epoch', 'save_total_limit': 1, 'auto_find_batch_size': False, 'mixed_precision': 'bf16', 'lr': 0.0001, 'epochs': 10, 'batch_size': 4, 'warmup_ratio': 0.1, 'gradient_accumulation': 4, 'optimizer': 'adamw_torch', 'scheduler': 'cosine', 'weight_decay': 0.0, 'max_grad_norm': 1.0, 'seed': 42, 'chat_template': None, 'quantization': 'int4', 'target_modules': 'q_proj,v_proj', 'merge_adapter': False, 'peft': True, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1, 'model_ref': None, 'dpo_beta': 0.1, 'max_prompt_length': 128, 'max_completion_length': None, 'prompt_text_column': 'autotrain_prompt', 'text_column': 'autotrain_text', 'rejected_text_column': 'autotrain_rejected_text', 'push_to_hub': True, 'username': 'VidyutCx', 'token': '*****', 'unsloth': False, 'distributed_backend': None}\u001b[0m\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1743433002.969192   17556 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1743433002.975616   17556 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-03-31 14:56:47\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_reward\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mStarting Reward training...\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-03-31 14:56:47\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m487\u001b[0m - \u001b[1mloading dataset from disk\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-03-31 14:56:47\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m550\u001b[0m - \u001b[1mTrain data: Dataset({\n",
            "    features: ['chosen', 'rejected'],\n",
            "    num_rows: 111\n",
            "})\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-03-31 14:56:47\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m551\u001b[0m - \u001b[1mValid data: None\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-03-31 14:56:47\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_logging_steps\u001b[0m:\u001b[36m671\u001b[0m - \u001b[1mconfiguring logging steps\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-03-31 14:56:47\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_logging_steps\u001b[0m:\u001b[36m684\u001b[0m - \u001b[1mLogging steps: 5\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-03-31 14:56:47\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_training_args\u001b[0m:\u001b[36m723\u001b[0m - \u001b[1mconfiguring training args\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-03-31 14:56:47\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_block_size\u001b[0m:\u001b[36m801\u001b[0m - \u001b[1mUsing block size 1024\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-03-31 14:56:47\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_reward\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mloading model config...\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-03-31 14:56:47\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_reward\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mloading model...\u001b[0m\n",
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
            "model.safetensors.index.json: 100% 23.9k/23.9k [00:00<00:00, 84.0MB/s]\n",
            "Downloading shards:   0% 0/3 [00:00<?, ?it/s]\n",
            "model-00001-of-00003.safetensors:   0% 0.00/4.95G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   0% 21.0M/4.95G [00:00<00:30, 162MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   1% 52.4M/4.95G [00:00<00:20, 240MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   2% 83.9M/4.95G [00:00<00:18, 264MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   2% 115M/4.95G [00:00<00:17, 276MB/s] \u001b[A\n",
            "model-00001-of-00003.safetensors:   3% 147M/4.95G [00:00<00:18, 264MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   4% 178M/4.95G [00:00<00:18, 264MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   4% 210M/4.95G [00:00<00:17, 264MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   5% 241M/4.95G [00:00<00:17, 262MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   6% 273M/4.95G [00:01<00:17, 268MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   6% 304M/4.95G [00:01<00:17, 265MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   7% 336M/4.95G [00:01<00:18, 252MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   7% 367M/4.95G [00:01<00:17, 259MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   8% 398M/4.95G [00:01<00:17, 254MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   9% 430M/4.95G [00:01<00:17, 253MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   9% 461M/4.95G [00:01<00:17, 251MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  10% 493M/4.95G [00:01<00:18, 246MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  11% 524M/4.95G [00:02<00:17, 253MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  11% 556M/4.95G [00:02<00:17, 251MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  12% 587M/4.95G [00:02<00:16, 258MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  12% 619M/4.95G [00:02<00:16, 265MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  13% 650M/4.95G [00:02<00:16, 254MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  14% 682M/4.95G [00:02<00:16, 252MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  14% 713M/4.95G [00:02<00:16, 254MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  15% 744M/4.95G [00:02<00:16, 254MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  16% 776M/4.95G [00:03<00:16, 255MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  16% 807M/4.95G [00:03<00:16, 249MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  17% 839M/4.95G [00:03<00:16, 249MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  18% 870M/4.95G [00:03<00:16, 245MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  18% 902M/4.95G [00:03<00:16, 245MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  19% 933M/4.95G [00:03<00:16, 250MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  19% 965M/4.95G [00:03<00:15, 253MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  20% 996M/4.95G [00:03<00:16, 241MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  21% 1.03G/4.95G [00:04<00:16, 231MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  21% 1.06G/4.95G [00:04<00:17, 217MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  22% 1.09G/4.95G [00:04<00:17, 219MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  23% 1.12G/4.95G [00:04<00:17, 213MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  23% 1.15G/4.95G [00:04<00:16, 224MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  24% 1.18G/4.95G [00:04<00:18, 208MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  25% 1.22G/4.95G [00:04<00:17, 213MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  25% 1.25G/4.95G [00:05<00:16, 224MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  26% 1.28G/4.95G [00:05<00:15, 231MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  26% 1.31G/4.95G [00:05<00:17, 210MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  27% 1.34G/4.95G [00:05<00:17, 208MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  28% 1.37G/4.95G [00:05<00:17, 206MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  28% 1.39G/4.95G [00:05<00:17, 207MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  29% 1.43G/4.95G [00:05<00:15, 223MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  29% 1.46G/4.95G [00:06<00:16, 213MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  30% 1.49G/4.95G [00:06<00:15, 223MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  31% 1.52G/4.95G [00:06<00:14, 230MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  31% 1.55G/4.95G [00:06<00:14, 235MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  32% 1.58G/4.95G [00:06<00:15, 221MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  33% 1.61G/4.95G [00:06<00:15, 212MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  33% 1.65G/4.95G [00:06<00:15, 219MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  34% 1.68G/4.95G [00:07<00:14, 225MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  35% 1.71G/4.95G [00:07<00:14, 228MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  35% 1.74G/4.95G [00:07<00:14, 224MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  36% 1.77G/4.95G [00:07<00:13, 234MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  36% 1.80G/4.95G [00:07<00:13, 240MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  37% 1.84G/4.95G [00:07<00:12, 243MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  38% 1.87G/4.95G [00:07<00:12, 253MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  38% 1.90G/4.95G [00:07<00:12, 250MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  39% 1.93G/4.95G [00:08<00:11, 257MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  40% 1.96G/4.95G [00:08<00:11, 255MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  40% 1.99G/4.95G [00:08<00:11, 251MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  41% 2.02G/4.95G [00:08<00:11, 251MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  42% 2.06G/4.95G [00:08<00:11, 249MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  42% 2.09G/4.95G [00:08<00:11, 251MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  43% 2.12G/4.95G [00:08<00:11, 254MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  43% 2.15G/4.95G [00:08<00:11, 251MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  44% 2.18G/4.95G [00:09<00:11, 245MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  45% 2.21G/4.95G [00:09<00:11, 247MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  45% 2.24G/4.95G [00:09<00:10, 250MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  46% 2.28G/4.95G [00:09<00:11, 242MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  47% 2.31G/4.95G [00:09<00:10, 246MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  47% 2.34G/4.95G [00:09<00:10, 248MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  48% 2.37G/4.95G [00:09<00:10, 245MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  49% 2.40G/4.95G [00:10<00:10, 243MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  49% 2.43G/4.95G [00:10<00:10, 245MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  50% 2.46G/4.95G [00:10<00:14, 172MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  50% 2.50G/4.95G [00:10<00:12, 198MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  51% 2.53G/4.95G [00:10<00:11, 206MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  52% 2.56G/4.95G [00:10<00:11, 216MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  52% 2.59G/4.95G [00:10<00:10, 225MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  53% 2.62G/4.95G [00:11<00:10, 230MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  54% 2.65G/4.95G [00:11<00:09, 235MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  54% 2.68G/4.95G [00:11<00:09, 239MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  55% 2.72G/4.95G [00:11<00:09, 244MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  56% 2.75G/4.95G [00:11<00:09, 240MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  56% 2.78G/4.95G [00:11<00:08, 245MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  57% 2.81G/4.95G [00:11<00:08, 249MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  57% 2.84G/4.95G [00:11<00:08, 249MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  58% 2.87G/4.95G [00:12<00:08, 251MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  59% 2.90G/4.95G [00:12<00:08, 254MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  59% 2.94G/4.95G [00:12<00:07, 254MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  60% 2.97G/4.95G [00:12<00:07, 265MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  61% 3.00G/4.95G [00:12<00:07, 256MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  61% 3.03G/4.95G [00:12<00:07, 258MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  62% 3.06G/4.95G [00:12<00:07, 257MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  62% 3.09G/4.95G [00:12<00:07, 253MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  63% 3.12G/4.95G [00:13<00:07, 257MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  64% 3.16G/4.95G [00:13<00:07, 250MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  64% 3.19G/4.95G [00:14<00:29, 58.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  65% 3.22G/4.95G [00:14<00:22, 76.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  66% 3.25G/4.95G [00:14<00:17, 97.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  66% 3.28G/4.95G [00:15<00:14, 119MB/s] \u001b[A\n",
            "model-00001-of-00003.safetensors:  67% 3.31G/4.95G [00:15<00:12, 134MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  68% 3.34G/4.95G [00:15<00:10, 155MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  68% 3.38G/4.95G [00:15<00:09, 174MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  69% 3.41G/4.95G [00:15<00:08, 189MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  69% 3.44G/4.95G [00:15<00:07, 201MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  70% 3.47G/4.95G [00:15<00:06, 214MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  71% 3.50G/4.95G [00:15<00:06, 222MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  71% 3.53G/4.95G [00:16<00:06, 230MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  72% 3.57G/4.95G [00:16<00:05, 238MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  73% 3.60G/4.95G [00:16<00:05, 246MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  73% 3.63G/4.95G [00:16<00:05, 246MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  74% 3.66G/4.95G [00:16<00:05, 237MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  75% 3.69G/4.95G [00:16<00:05, 236MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  75% 3.72G/4.95G [00:16<00:05, 226MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  76% 3.75G/4.95G [00:17<00:05, 225MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  76% 3.79G/4.95G [00:17<00:05, 231MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  77% 3.82G/4.95G [00:17<00:05, 223MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  78% 3.85G/4.95G [00:17<00:04, 230MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  78% 3.88G/4.95G [00:17<00:04, 241MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  79% 3.91G/4.95G [00:17<00:04, 230MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  80% 3.94G/4.95G [00:17<00:04, 201MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  80% 3.97G/4.95G [00:18<00:05, 187MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  81% 4.00G/4.95G [00:18<00:05, 187MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  81% 4.03G/4.95G [00:18<00:04, 199MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  82% 4.06G/4.95G [00:18<00:04, 203MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  83% 4.09G/4.95G [00:18<00:04, 210MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  83% 4.12G/4.95G [00:18<00:04, 206MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  84% 4.15G/4.95G [00:18<00:03, 207MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  84% 4.17G/4.95G [00:19<00:03, 203MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  85% 4.19G/4.95G [00:19<00:03, 199MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  85% 4.22G/4.95G [00:19<00:03, 197MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  86% 4.24G/4.95G [00:22<00:35, 20.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  86% 4.27G/4.95G [00:22<00:22, 30.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  87% 4.30G/4.95G [00:23<00:14, 44.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  87% 4.33G/4.95G [00:23<00:09, 61.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  88% 4.36G/4.95G [00:23<00:07, 81.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  89% 4.39G/4.95G [00:23<00:05, 103MB/s] \u001b[A\n",
            "model-00001-of-00003.safetensors:  89% 4.42G/4.95G [00:23<00:04, 126MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  90% 4.46G/4.95G [00:23<00:03, 149MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  91% 4.49G/4.95G [00:23<00:02, 171MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  91% 4.52G/4.95G [00:23<00:02, 186MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  92% 4.55G/4.95G [00:24<00:02, 198MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  93% 4.58G/4.95G [00:24<00:01, 216MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  93% 4.61G/4.95G [00:24<00:01, 227MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  94% 4.65G/4.95G [00:24<00:01, 243MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  94% 4.68G/4.95G [00:24<00:01, 257MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  95% 4.71G/4.95G [00:24<00:00, 269MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  96% 4.74G/4.95G [00:24<00:00, 276MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  96% 4.77G/4.95G [00:24<00:00, 249MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  97% 4.80G/4.95G [00:25<00:00, 225MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  98% 4.83G/4.95G [00:25<00:00, 229MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  98% 4.87G/4.95G [00:25<00:00, 232MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  99% 4.90G/4.95G [00:25<00:00, 237MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors: 100% 4.95G/4.95G [00:25<00:00, 193MB/s]\n",
            "Downloading shards:  33% 1/3 [00:25<00:51, 25.80s/it]\n",
            "model-00002-of-00003.safetensors:   0% 0.00/5.00G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   1% 31.5M/5.00G [00:00<00:16, 303MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   1% 62.9M/5.00G [00:00<00:16, 291MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   2% 94.4M/5.00G [00:00<00:16, 289MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   3% 126M/5.00G [00:00<00:19, 255MB/s] \u001b[A\n",
            "model-00002-of-00003.safetensors:   3% 157M/5.00G [00:00<00:19, 243MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   4% 189M/5.00G [00:00<00:20, 232MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   4% 220M/5.00G [00:00<00:20, 233MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   5% 252M/5.00G [00:01<00:46, 102MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   5% 273M/5.00G [00:01<00:41, 114MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   6% 294M/5.00G [00:01<00:37, 125MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   7% 325M/5.00G [00:01<00:31, 148MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   7% 357M/5.00G [00:02<00:27, 170MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   8% 388M/5.00G [00:02<00:24, 192MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   8% 419M/5.00G [00:02<00:22, 202MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   9% 451M/5.00G [00:02<00:21, 213MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  10% 482M/5.00G [00:02<00:20, 221MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  10% 514M/5.00G [00:02<00:19, 232MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  11% 545M/5.00G [00:02<00:18, 235MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  12% 577M/5.00G [00:02<00:18, 242MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  12% 608M/5.00G [00:03<00:17, 246MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  13% 640M/5.00G [00:03<00:17, 248MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  13% 671M/5.00G [00:03<00:17, 242MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  14% 703M/5.00G [00:03<00:17, 245MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  15% 734M/5.00G [00:03<00:16, 254MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  15% 765M/5.00G [00:03<00:16, 255MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  16% 797M/5.00G [00:03<00:16, 256MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  17% 828M/5.00G [00:03<00:16, 247MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  17% 860M/5.00G [00:04<00:17, 233MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  18% 891M/5.00G [00:04<00:18, 219MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  18% 923M/5.00G [00:04<00:19, 214MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  19% 954M/5.00G [00:04<00:18, 213MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  20% 986M/5.00G [00:04<00:19, 207MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  20% 1.02G/5.00G [00:04<00:18, 212MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  21% 1.05G/5.00G [00:05<00:19, 202MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  21% 1.07G/5.00G [00:05<00:19, 202MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  22% 1.10G/5.00G [00:05<00:18, 213MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  23% 1.13G/5.00G [00:05<00:18, 214MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  23% 1.16G/5.00G [00:05<00:17, 214MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  24% 1.20G/5.00G [00:05<00:18, 206MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  25% 1.23G/5.00G [00:05<00:16, 222MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  25% 1.26G/5.00G [00:05<00:16, 232MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  26% 1.29G/5.00G [00:06<00:17, 217MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  26% 1.32G/5.00G [00:06<00:18, 200MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  27% 1.34G/5.00G [00:06<00:18, 195MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  27% 1.37G/5.00G [00:06<00:17, 203MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  28% 1.39G/5.00G [00:06<00:17, 204MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  29% 1.43G/5.00G [00:06<00:16, 212MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  29% 1.46G/5.00G [00:06<00:16, 220MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  30% 1.49G/5.00G [00:07<00:15, 224MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  30% 1.52G/5.00G [00:07<00:14, 238MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  31% 1.55G/5.00G [00:07<00:14, 244MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  32% 1.58G/5.00G [00:07<00:13, 255MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  32% 1.61G/5.00G [00:07<00:13, 257MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  33% 1.65G/5.00G [00:07<00:12, 264MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  34% 1.68G/5.00G [00:07<00:12, 269MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  34% 1.71G/5.00G [00:07<00:12, 263MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  35% 1.74G/5.00G [00:08<00:12, 265MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  35% 1.77G/5.00G [00:08<00:12, 264MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  36% 1.80G/5.00G [00:08<00:12, 257MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  37% 1.84G/5.00G [00:08<00:12, 252MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  37% 1.87G/5.00G [00:08<00:12, 249MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  38% 1.90G/5.00G [00:08<00:12, 256MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  39% 1.93G/5.00G [00:08<00:12, 253MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  39% 1.96G/5.00G [00:08<00:11, 256MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  40% 1.99G/5.00G [00:09<00:11, 258MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  40% 2.02G/5.00G [00:09<00:11, 261MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  41% 2.06G/5.00G [00:09<00:11, 262MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  42% 2.09G/5.00G [00:09<00:11, 260MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  42% 2.12G/5.00G [00:09<00:11, 256MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  43% 2.15G/5.00G [00:09<00:11, 250MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  44% 2.18G/5.00G [00:09<00:11, 252MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  44% 2.21G/5.00G [00:09<00:11, 252MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  45% 2.24G/5.00G [00:10<00:11, 244MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  46% 2.28G/5.00G [00:10<00:11, 247MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  46% 2.31G/5.00G [00:10<00:10, 245MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  47% 2.34G/5.00G [00:10<00:10, 256MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  47% 2.37G/5.00G [00:10<00:10, 250MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  48% 2.40G/5.00G [00:10<00:10, 245MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  49% 2.43G/5.00G [00:10<00:10, 250MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  49% 2.46G/5.00G [00:10<00:09, 255MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  50% 2.50G/5.00G [00:11<00:09, 251MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  51% 2.53G/5.00G [00:11<00:09, 258MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  51% 2.56G/5.00G [00:11<00:09, 258MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  52% 2.59G/5.00G [00:11<00:09, 261MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  52% 2.62G/5.00G [00:11<00:09, 254MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  53% 2.65G/5.00G [00:11<00:09, 244MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  54% 2.68G/5.00G [00:11<00:09, 251MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  54% 2.72G/5.00G [00:11<00:09, 253MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  55% 2.75G/5.00G [00:12<00:08, 257MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  56% 2.78G/5.00G [00:12<00:08, 257MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  56% 2.81G/5.00G [00:12<00:08, 253MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  57% 2.84G/5.00G [00:12<00:08, 256MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  57% 2.87G/5.00G [00:12<00:08, 257MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  58% 2.90G/5.00G [00:12<00:08, 253MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  59% 2.94G/5.00G [00:12<00:08, 252MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  59% 2.97G/5.00G [00:12<00:07, 256MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  60% 3.00G/5.00G [00:13<00:07, 255MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  61% 3.03G/5.00G [00:13<00:07, 257MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  61% 3.06G/5.00G [00:13<00:07, 258MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  62% 3.09G/5.00G [00:13<00:07, 259MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  62% 3.12G/5.00G [00:13<00:07, 259MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  63% 3.16G/5.00G [00:13<00:07, 256MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  64% 3.19G/5.00G [00:13<00:07, 249MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  64% 3.22G/5.00G [00:13<00:07, 250MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  65% 3.25G/5.00G [00:14<00:06, 250MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  66% 3.28G/5.00G [00:14<00:07, 242MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  66% 3.31G/5.00G [00:14<00:06, 246MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  67% 3.34G/5.00G [00:14<00:06, 249MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  68% 3.38G/5.00G [00:14<00:06, 239MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  68% 3.41G/5.00G [00:14<00:06, 237MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  69% 3.44G/5.00G [00:14<00:06, 239MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  69% 3.47G/5.00G [00:14<00:06, 239MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  70% 3.50G/5.00G [00:15<00:06, 238MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  71% 3.53G/5.00G [00:15<00:06, 240MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  71% 3.57G/5.00G [00:15<00:05, 244MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  72% 3.60G/5.00G [00:15<00:05, 248MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  73% 3.63G/5.00G [00:15<00:05, 245MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  73% 3.66G/5.00G [00:15<00:05, 246MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  74% 3.69G/5.00G [00:15<00:05, 249MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  74% 3.72G/5.00G [00:15<00:05, 247MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  75% 3.75G/5.00G [00:16<00:06, 195MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  76% 3.79G/5.00G [00:16<00:07, 170MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  76% 3.82G/5.00G [00:16<00:06, 177MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  77% 3.85G/5.00G [00:16<00:06, 188MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  77% 3.87G/5.00G [00:16<00:05, 191MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  78% 3.90G/5.00G [00:16<00:05, 204MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  79% 3.93G/5.00G [00:17<00:05, 199MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  79% 3.95G/5.00G [00:17<00:07, 146MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  79% 3.97G/5.00G [00:17<00:08, 124MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  80% 4.00G/5.00G [00:17<00:09, 108MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  80% 4.02G/5.00G [00:18<00:08, 110MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  81% 4.04G/5.00G [00:18<00:08, 120MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  81% 4.06G/5.00G [00:18<00:07, 132MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  82% 4.08G/5.00G [00:18<00:06, 143MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  82% 4.10G/5.00G [00:18<00:05, 155MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  82% 4.12G/5.00G [00:23<01:06, 13.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  83% 4.15G/5.00G [00:23<00:40, 20.9MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  84% 4.18G/5.00G [00:23<00:26, 31.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  84% 4.22G/5.00G [00:23<00:17, 44.8MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  85% 4.25G/5.00G [00:24<00:12, 59.6MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  86% 4.28G/5.00G [00:24<00:09, 78.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  86% 4.31G/5.00G [00:24<00:06, 100MB/s] \u001b[A\n",
            "model-00002-of-00003.safetensors:  87% 4.34G/5.00G [00:24<00:05, 123MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  87% 4.37G/5.00G [00:24<00:04, 145MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  88% 4.40G/5.00G [00:24<00:03, 166MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  89% 4.44G/5.00G [00:24<00:03, 187MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  89% 4.47G/5.00G [00:24<00:02, 206MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  90% 4.50G/5.00G [00:25<00:02, 205MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  91% 4.53G/5.00G [00:25<00:02, 216MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  91% 4.56G/5.00G [00:25<00:02, 219MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  92% 4.59G/5.00G [00:25<00:01, 238MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  92% 4.62G/5.00G [00:25<00:01, 240MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  93% 4.66G/5.00G [00:25<00:01, 246MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  94% 4.69G/5.00G [00:25<00:01, 246MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  94% 4.72G/5.00G [00:26<00:01, 244MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  95% 4.75G/5.00G [00:26<00:01, 242MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  96% 4.78G/5.00G [00:26<00:00, 246MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  96% 4.81G/5.00G [00:26<00:00, 248MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  97% 4.84G/5.00G [00:26<00:00, 248MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  98% 4.88G/5.00G [00:26<00:00, 245MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  98% 4.91G/5.00G [00:26<00:00, 257MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  99% 4.94G/5.00G [00:26<00:00, 257MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  99% 4.97G/5.00G [00:27<00:00, 257MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors: 100% 5.00G/5.00G [00:27<00:00, 184MB/s]\n",
            "Downloading shards:  67% 2/3 [00:53<00:26, 26.69s/it]\n",
            "model-00003-of-00003.safetensors:   0% 0.00/4.55G [00:00<?, ?B/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   0% 21.0M/4.55G [00:00<00:24, 187MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   1% 41.9M/4.55G [00:00<00:23, 191MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   2% 73.4M/4.55G [00:00<00:20, 219MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   2% 105M/4.55G [00:00<00:18, 239MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:   3% 136M/4.55G [00:00<00:18, 240MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   4% 168M/4.55G [00:00<00:18, 236MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   4% 199M/4.55G [00:00<00:18, 239MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   5% 231M/4.55G [00:00<00:18, 239MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   6% 262M/4.55G [00:01<00:17, 245MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   6% 294M/4.55G [00:01<00:17, 249MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   7% 325M/4.55G [00:01<00:16, 263MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   8% 357M/4.55G [00:01<00:15, 274MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   9% 388M/4.55G [00:01<00:14, 285MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   9% 419M/4.55G [00:01<00:14, 289MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  10% 451M/4.55G [00:02<00:31, 132MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  11% 482M/4.55G [00:02<00:27, 147MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  11% 514M/4.55G [00:03<00:50, 80.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  12% 535M/4.55G [00:03<00:57, 70.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  12% 556M/4.55G [00:03<00:53, 75.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  13% 587M/4.55G [00:03<00:40, 98.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  14% 619M/4.55G [00:04<00:32, 122MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:  14% 650M/4.55G [00:04<00:26, 149MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  15% 682M/4.55G [00:04<00:22, 169MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  16% 713M/4.55G [00:05<00:43, 88.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  16% 744M/4.55G [00:05<00:34, 111MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:  17% 776M/4.55G [00:05<00:27, 135MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  18% 807M/4.55G [00:05<00:23, 158MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  18% 839M/4.55G [00:05<00:20, 178MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  19% 870M/4.55G [00:05<00:18, 197MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  20% 902M/4.55G [00:05<00:17, 213MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  21% 933M/4.55G [00:05<00:16, 224MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  21% 965M/4.55G [00:06<00:15, 231MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  22% 996M/4.55G [00:06<00:15, 234MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  23% 1.03G/4.55G [00:06<00:14, 242MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  23% 1.06G/4.55G [00:06<00:14, 242MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  24% 1.09G/4.55G [00:06<00:13, 258MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  25% 1.12G/4.55G [00:06<00:12, 267MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  25% 1.15G/4.55G [00:06<00:13, 261MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  26% 1.18G/4.55G [00:06<00:13, 257MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  27% 1.22G/4.55G [00:06<00:12, 262MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  27% 1.25G/4.55G [00:07<00:12, 255MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  28% 1.28G/4.55G [00:07<00:12, 259MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  29% 1.31G/4.55G [00:07<00:13, 244MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  30% 1.34G/4.55G [00:07<00:13, 239MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  30% 1.37G/4.55G [00:07<00:13, 237MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  31% 1.41G/4.55G [00:07<00:12, 246MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  32% 1.44G/4.55G [00:07<00:12, 248MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  32% 1.47G/4.55G [00:08<00:12, 247MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  33% 1.50G/4.55G [00:08<00:12, 241MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  34% 1.53G/4.55G [00:08<00:12, 249MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  34% 1.56G/4.55G [00:08<00:11, 250MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  35% 1.59G/4.55G [00:08<00:12, 240MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  36% 1.63G/4.55G [00:08<00:11, 245MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  36% 1.66G/4.55G [00:08<00:11, 241MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  37% 1.69G/4.55G [00:08<00:11, 251MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  38% 1.72G/4.55G [00:09<00:11, 249MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  39% 1.75G/4.55G [00:09<00:10, 256MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  39% 1.78G/4.55G [00:09<00:10, 259MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  40% 1.81G/4.55G [00:09<00:10, 251MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  41% 1.85G/4.55G [00:09<00:10, 246MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  41% 1.88G/4.55G [00:09<00:10, 256MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  42% 1.91G/4.55G [00:09<00:10, 254MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  43% 1.94G/4.55G [00:09<00:10, 246MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  43% 1.97G/4.55G [00:10<00:10, 248MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  44% 2.00G/4.55G [00:10<00:10, 237MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  45% 2.03G/4.55G [00:10<00:10, 240MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  45% 2.07G/4.55G [00:10<00:11, 225MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  46% 2.10G/4.55G [00:10<00:10, 227MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  47% 2.13G/4.55G [00:10<00:10, 233MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  48% 2.16G/4.55G [00:10<00:09, 244MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  48% 2.19G/4.55G [00:10<00:09, 252MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  49% 2.22G/4.55G [00:11<00:09, 252MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  50% 2.25G/4.55G [00:11<00:09, 245MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  50% 2.29G/4.55G [00:11<00:08, 253MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  51% 2.32G/4.55G [00:11<00:08, 257MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  52% 2.35G/4.55G [00:11<00:08, 262MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  52% 2.38G/4.55G [00:11<00:08, 259MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  53% 2.41G/4.55G [00:11<00:08, 264MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  54% 2.44G/4.55G [00:11<00:07, 269MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  54% 2.47G/4.55G [00:12<00:07, 263MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  55% 2.51G/4.55G [00:12<00:07, 260MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  56% 2.54G/4.55G [00:12<00:07, 253MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  57% 2.57G/4.55G [00:12<00:07, 259MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  57% 2.60G/4.55G [00:12<00:07, 260MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  58% 2.63G/4.55G [00:12<00:07, 253MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  59% 2.66G/4.55G [00:12<00:07, 263MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  59% 2.69G/4.55G [00:12<00:07, 251MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  60% 2.73G/4.55G [00:13<00:07, 247MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  61% 2.76G/4.55G [00:13<00:07, 253MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  61% 2.79G/4.55G [00:13<00:10, 170MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  62% 2.82G/4.55G [00:13<00:08, 196MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  63% 2.85G/4.55G [00:13<00:07, 218MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  63% 2.88G/4.55G [00:13<00:07, 225MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  64% 2.92G/4.55G [00:13<00:07, 226MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  65% 2.95G/4.55G [00:14<00:07, 207MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  65% 2.98G/4.55G [00:14<00:07, 218MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  66% 3.01G/4.55G [00:14<00:07, 219MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  67% 3.04G/4.55G [00:14<00:07, 210MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  68% 3.07G/4.55G [00:14<00:07, 195MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  68% 3.10G/4.55G [00:14<00:07, 202MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  69% 3.14G/4.55G [00:15<00:06, 210MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  70% 3.17G/4.55G [00:15<00:06, 205MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  70% 3.20G/4.55G [00:15<00:06, 213MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  71% 3.23G/4.55G [00:15<00:06, 219MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  72% 3.26G/4.55G [00:15<00:05, 219MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  72% 3.29G/4.55G [00:15<00:06, 196MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  73% 3.31G/4.55G [00:15<00:06, 195MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  73% 3.33G/4.55G [00:16<00:06, 192MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  74% 3.36G/4.55G [00:16<00:06, 195MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  74% 3.38G/4.55G [00:16<00:06, 180MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  75% 3.40G/4.55G [00:16<00:06, 181MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  75% 3.42G/4.55G [00:16<00:05, 188MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  76% 3.45G/4.55G [00:16<00:05, 204MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  77% 3.48G/4.55G [00:16<00:04, 224MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  77% 3.51G/4.55G [00:16<00:04, 245MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  78% 3.54G/4.55G [00:16<00:03, 258MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  79% 3.58G/4.55G [00:17<00:03, 246MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  79% 3.61G/4.55G [00:17<00:04, 214MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  80% 3.64G/4.55G [00:17<00:04, 200MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  80% 3.66G/4.55G [00:17<00:04, 195MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  81% 3.69G/4.55G [00:17<00:04, 209MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  82% 3.72G/4.55G [00:17<00:03, 210MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  83% 3.75G/4.55G [00:17<00:03, 221MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  83% 3.79G/4.55G [00:18<00:03, 232MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  84% 3.82G/4.55G [00:18<00:03, 236MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  85% 3.85G/4.55G [00:18<00:02, 248MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  85% 3.88G/4.55G [00:18<00:02, 251MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  86% 3.91G/4.55G [00:18<00:02, 256MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  87% 3.94G/4.55G [00:18<00:02, 235MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  87% 3.97G/4.55G [00:18<00:02, 242MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  88% 4.01G/4.55G [00:18<00:02, 249MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  89% 4.04G/4.55G [00:19<00:02, 249MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  89% 4.07G/4.55G [00:19<00:01, 249MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  90% 4.10G/4.55G [00:19<00:01, 243MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  91% 4.13G/4.55G [00:19<00:01, 233MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  92% 4.16G/4.55G [00:19<00:01, 234MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  92% 4.19G/4.55G [00:19<00:01, 243MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  93% 4.23G/4.55G [00:19<00:01, 245MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  94% 4.26G/4.55G [00:20<00:01, 245MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  94% 4.29G/4.55G [00:20<00:01, 245MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  95% 4.32G/4.55G [00:20<00:00, 251MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  96% 4.35G/4.55G [00:20<00:00, 257MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  96% 4.38G/4.55G [00:20<00:00, 256MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  97% 4.41G/4.55G [00:20<00:00, 228MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  98% 4.45G/4.55G [00:20<00:00, 235MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  98% 4.48G/4.55G [00:20<00:00, 245MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  99% 4.51G/4.55G [00:21<00:00, 255MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors: 100% 4.55G/4.55G [00:21<00:00, 214MB/s]\n",
            "Downloading shards: 100% 3/3 [01:14<00:00, 24.81s/it]\n",
            "Loading checkpoint shards: 100% 3/3 [01:05<00:00, 21.84s/it]\n",
            "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.3 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-03-31 14:59:08\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_reward\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1mmodel dtype: torch.float16\u001b[0m\n",
            "Running tokenizer on train dataset (num_proc=4): 100% 111/111 [00:01<00:00, 109.52 examples/s]\n",
            "Filter: 100% 111/111 [00:00<00:00, 1235.09 examples/s]\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-03-31 14:59:10\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_reward\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mcreating trainer\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-03-31 14:59:11\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_train_begin\u001b[0m:\u001b[36m386\u001b[0m - \u001b[1mStarting to train...\u001b[0m\n",
            "  0% 0/70 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "  7% 5/70 [11:25<2:24:22, 133.27s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2025-03-31 15:10:37\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 5.4262, 'grad_norm': 193.54782104492188, 'learning_rate': 7.142857142857143e-05, 'epoch': 0.7142857142857143}\u001b[0m\n",
            "{'loss': 5.4262, 'grad_norm': 193.54782104492188, 'learning_rate': 7.142857142857143e-05, 'epoch': 0.71}\n",
            " 14% 10/70 [22:34<2:13:15, 133.26s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2025-03-31 15:21:45\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.0389, 'grad_norm': 0.008106401190161705, 'learning_rate': 9.944154131125642e-05, 'epoch': 1.4285714285714286}\u001b[0m\n",
            "{'loss': 0.0389, 'grad_norm': 0.008106401190161705, 'learning_rate': 9.944154131125642e-05, 'epoch': 1.43}\n",
            " 21% 15/70 [34:27<2:10:02, 141.86s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2025-03-31 15:33:39\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.0, 'grad_norm': 1.6596259414490555e-13, 'learning_rate': 9.607381059352038e-05, 'epoch': 2.142857142857143}\u001b[0m\n",
            "{'loss': 0.0, 'grad_norm': 1.6596259414490555e-13, 'learning_rate': 9.607381059352038e-05, 'epoch': 2.14}\n",
            " 29% 20/70 [45:39<1:52:03, 134.46s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2025-03-31 15:44:51\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.0, 'grad_norm': 4.612797087730029e-18, 'learning_rate': 8.985662536114613e-05, 'epoch': 2.857142857142857}\u001b[0m\n",
            "{'loss': 0.0, 'grad_norm': 4.612797087730029e-18, 'learning_rate': 8.985662536114613e-05, 'epoch': 2.86}\n",
            " 31% 22/70 [50:15<1:49:07, 136.40s/it]"
          ]
        }
      ],
      "source": [
        "!autotrain llm --train \\\n",
        "    --project-name mistral-prompttune\\\n",
        "    --model mistralai/Mistral-7B-Instruct-v0.3 \\\n",
        "    --data-path . \\\n",
        "    --peft \\\n",
        "    --text-column text \\\n",
        "    --rejected-text-column rejected_text \\\n",
        "    --lora-r 16 \\\n",
        "    --lora-alpha 32 \\\n",
        "    --lora-dropout 0.1 \\\n",
        "    --quantization int4 \\\n",
        "    --lr 1e-4 \\\n",
        "    --batch-size 4 \\\n",
        "    --epochs 10 \\\n",
        "    --trainer reward \\\n",
        "    --scheduler cosine \\\n",
        "    --mixed-precision bf16 \\\n",
        "    --target-modules q_proj,v_proj \\\n",
        "    --push-to-hub \\\n",
        "    --username VidyutCx \\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwCwtAsAn1Bs",
        "outputId": "99ef4a8d-1c52-42bc-d999-b130966f7fd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usage: autotrain <command> [<args>] llm [-h] [--train] [--deploy] [--inference]\n",
            "                                        [--backend BACKEND] [--model MODEL]\n",
            "                                        [--project-name PROJECT_NAME] [--data-path DATA_PATH]\n",
            "                                        [--train-split TRAIN_SPLIT] [--valid-split VALID_SPLIT]\n",
            "                                        [--add-eos-token] [--model-max-length MODEL_MAX_LENGTH]\n",
            "                                        [--padding PADDING] [--trainer TRAINER]\n",
            "                                        [--use-flash-attention-2] [--log LOG]\n",
            "                                        [--disable-gradient-checkpointing]\n",
            "                                        [--logging-steps LOGGING_STEPS]\n",
            "                                        [--eval-strategy EVAL_STRATEGY]\n",
            "                                        [--save-total-limit SAVE_TOTAL_LIMIT]\n",
            "                                        [--auto-find-batch-size]\n",
            "                                        [--mixed-precision MIXED_PRECISION] [--lr LR]\n",
            "                                        [--epochs EPOCHS] [--batch-size BATCH_SIZE]\n",
            "                                        [--warmup-ratio WARMUP_RATIO]\n",
            "                                        [--gradient-accumulation GRADIENT_ACCUMULATION]\n",
            "                                        [--optimizer OPTIMIZER] [--scheduler SCHEDULER]\n",
            "                                        [--weight-decay WEIGHT_DECAY]\n",
            "                                        [--max-grad-norm MAX_GRAD_NORM] [--seed SEED]\n",
            "                                        [--chat-template CHAT_TEMPLATE]\n",
            "                                        [--quantization QUANTIZATION]\n",
            "                                        [--target-modules TARGET_MODULES] [--merge-adapter]\n",
            "                                        [--peft] [--lora-r LORA_R] [--lora-alpha LORA_ALPHA]\n",
            "                                        [--lora-dropout LORA_DROPOUT] [--model-ref MODEL_REF]\n",
            "                                        [--dpo-beta DPO_BETA]\n",
            "                                        [--max-prompt-length MAX_PROMPT_LENGTH]\n",
            "                                        [--max-completion-length MAX_COMPLETION_LENGTH]\n",
            "                                        [--prompt-text-column PROMPT_TEXT_COLUMN]\n",
            "                                        [--text-column TEXT_COLUMN]\n",
            "                                        [--rejected-text-column REJECTED_TEXT_COLUMN]\n",
            "                                        [--push-to-hub] [--username USERNAME] [--token TOKEN]\n",
            "                                        [--unsloth] [--distributed-backend DISTRIBUTED_BACKEND]\n",
            "                                        [--block_size BLOCK_SIZE]\n",
            "\n",
            "✨ Run AutoTrain LLM\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --train               Command to train the model\n",
            "  --deploy              Command to deploy the model (limited availability)\n",
            "  --inference           Command to run inference (limited availability)\n",
            "  --backend BACKEND     Backend\n",
            "  --model MODEL, --model MODEL, --model MODEL\n",
            "                        Model name to be used for training\n",
            "  --project-name PROJECT_NAME, --project_name PROJECT_NAME, --project-name PROJECT_NAME\n",
            "                        Name of the project and output directory\n",
            "  --data-path DATA_PATH, --data_path DATA_PATH, --data-path DATA_PATH\n",
            "                        Path to the dataset\n",
            "  --train-split TRAIN_SPLIT, --train_split TRAIN_SPLIT, --train-split TRAIN_SPLIT\n",
            "                        Configuration for the training data split\n",
            "  --valid-split VALID_SPLIT, --valid_split VALID_SPLIT, --valid-split VALID_SPLIT\n",
            "                        Configuration for the validation data split\n",
            "  --add-eos-token, --add_eos_token, --add-eos-token\n",
            "                        Whether to add an EOS token at the end of sequences\n",
            "  --model-max-length MODEL_MAX_LENGTH, --model_max_length MODEL_MAX_LENGTH, --model-max-length MODEL_MAX_LENGTH\n",
            "                        Maximum length of the model input\n",
            "  --padding PADDING, --padding PADDING, --padding PADDING\n",
            "                        Side on which to pad sequences (left or right)\n",
            "  --trainer TRAINER, --trainer TRAINER, --trainer TRAINER\n",
            "                        Type of trainer to use\n",
            "  --use-flash-attention-2, --use_flash_attention_2, --use-flash-attention-2\n",
            "                        Whether to use flash attention version 2\n",
            "  --log LOG, --log LOG, --log LOG\n",
            "                        Logging method for experiment tracking\n",
            "  --disable-gradient-checkpointing, --disable_gradient_checkpointing, --disable-gradient-checkpointing\n",
            "                        Whether to disable gradient checkpointing\n",
            "  --logging-steps LOGGING_STEPS, --logging_steps LOGGING_STEPS, --logging-steps LOGGING_STEPS\n",
            "                        Number of steps between logging events\n",
            "  --eval-strategy EVAL_STRATEGY, --eval_strategy EVAL_STRATEGY, --eval-strategy EVAL_STRATEGY\n",
            "                        Strategy for evaluation (e.g., 'epoch')\n",
            "  --save-total-limit SAVE_TOTAL_LIMIT, --save_total_limit SAVE_TOTAL_LIMIT, --save-total-limit SAVE_TOTAL_LIMIT\n",
            "                        Maximum number of checkpoints to keep\n",
            "  --auto-find-batch-size, --auto_find_batch_size, --auto-find-batch-size\n",
            "                        Whether to automatically find the optimal batch size\n",
            "  --mixed-precision MIXED_PRECISION, --mixed_precision MIXED_PRECISION, --mixed-precision MIXED_PRECISION\n",
            "                        Type of mixed precision to use (e.g., 'fp16', 'bf16', or None)\n",
            "  --lr LR, --lr LR, --lr LR\n",
            "                        Learning rate for training\n",
            "  --epochs EPOCHS, --epochs EPOCHS, --epochs EPOCHS\n",
            "                        Number of training epochs\n",
            "  --batch-size BATCH_SIZE, --batch_size BATCH_SIZE, --batch-size BATCH_SIZE\n",
            "                        Batch size for training\n",
            "  --warmup-ratio WARMUP_RATIO, --warmup_ratio WARMUP_RATIO, --warmup-ratio WARMUP_RATIO\n",
            "                        Proportion of training to perform learning rate warmup\n",
            "  --gradient-accumulation GRADIENT_ACCUMULATION, --gradient_accumulation GRADIENT_ACCUMULATION, --gradient-accumulation GRADIENT_ACCUMULATION\n",
            "                        Number of steps to accumulate gradients before updating\n",
            "  --optimizer OPTIMIZER, --optimizer OPTIMIZER, --optimizer OPTIMIZER\n",
            "                        Optimizer to use for training\n",
            "  --scheduler SCHEDULER, --scheduler SCHEDULER, --scheduler SCHEDULER\n",
            "                        Learning rate scheduler to use\n",
            "  --weight-decay WEIGHT_DECAY, --weight_decay WEIGHT_DECAY, --weight-decay WEIGHT_DECAY\n",
            "                        Weight decay to apply to the optimizer\n",
            "  --max-grad-norm MAX_GRAD_NORM, --max_grad_norm MAX_GRAD_NORM, --max-grad-norm MAX_GRAD_NORM\n",
            "                        Maximum norm for gradient clipping\n",
            "  --seed SEED, --seed SEED, --seed SEED\n",
            "                        Random seed for reproducibility\n",
            "  --chat-template CHAT_TEMPLATE, --chat_template CHAT_TEMPLATE, --chat-template CHAT_TEMPLATE\n",
            "                        Template for chat-based models, options include: None, zephyr, chatml, or\n",
            "                        tokenizer\n",
            "  --quantization QUANTIZATION, --quantization QUANTIZATION, --quantization QUANTIZATION\n",
            "                        Quantization method to use (e.g., 'int4', 'int8', or None)\n",
            "  --target-modules TARGET_MODULES, --target_modules TARGET_MODULES, --target-modules TARGET_MODULES\n",
            "                        Target modules for quantization or fine-tuning\n",
            "  --merge-adapter, --merge_adapter, --merge-adapter\n",
            "                        Whether to merge the adapter layers\n",
            "  --peft, --peft, --peft\n",
            "                        Whether to use Parameter-Efficient Fine-Tuning (PEFT)\n",
            "  --lora-r LORA_R, --lora_r LORA_R, --lora-r LORA_R\n",
            "                        Rank of the LoRA matrices\n",
            "  --lora-alpha LORA_ALPHA, --lora_alpha LORA_ALPHA, --lora-alpha LORA_ALPHA\n",
            "                        Alpha parameter for LoRA\n",
            "  --lora-dropout LORA_DROPOUT, --lora_dropout LORA_DROPOUT, --lora-dropout LORA_DROPOUT\n",
            "                        Dropout rate for LoRA\n",
            "  --model-ref MODEL_REF, --model_ref MODEL_REF, --model-ref MODEL_REF\n",
            "                        Reference model for DPO trainer\n",
            "  --dpo-beta DPO_BETA, --dpo_beta DPO_BETA, --dpo-beta DPO_BETA\n",
            "                        Beta parameter for DPO trainer\n",
            "  --max-prompt-length MAX_PROMPT_LENGTH, --max_prompt_length MAX_PROMPT_LENGTH, --max-prompt-length MAX_PROMPT_LENGTH\n",
            "                        Maximum length of the prompt\n",
            "  --max-completion-length MAX_COMPLETION_LENGTH, --max_completion_length MAX_COMPLETION_LENGTH, --max-completion-length MAX_COMPLETION_LENGTH\n",
            "                        Maximum length of the completion\n",
            "  --prompt-text-column PROMPT_TEXT_COLUMN, --prompt_text_column PROMPT_TEXT_COLUMN, --prompt-text-column PROMPT_TEXT_COLUMN\n",
            "                        Column name for the prompt text\n",
            "  --text-column TEXT_COLUMN, --text_column TEXT_COLUMN, --text-column TEXT_COLUMN\n",
            "                        Column name for the text data\n",
            "  --rejected-text-column REJECTED_TEXT_COLUMN, --rejected_text_column REJECTED_TEXT_COLUMN, --rejected-text-column REJECTED_TEXT_COLUMN\n",
            "                        Column name for the rejected text data\n",
            "  --push-to-hub, --push_to_hub, --push-to-hub\n",
            "                        Whether to push the model to the Hugging Face Hub\n",
            "  --username USERNAME, --username USERNAME, --username USERNAME\n",
            "                        Hugging Face username for authentication\n",
            "  --token TOKEN, --token TOKEN, --token TOKEN\n",
            "                        Hugging Face token for authentication\n",
            "  --unsloth, --unsloth, --unsloth\n",
            "                        Whether to use the unsloth library\n",
            "  --distributed-backend DISTRIBUTED_BACKEND, --distributed_backend DISTRIBUTED_BACKEND, --distributed-backend DISTRIBUTED_BACKEND\n",
            "                        Backend to use for distributed training\n",
            "  --block_size BLOCK_SIZE, --block-size BLOCK_SIZE\n",
            "                        Block size\n"
          ]
        }
      ],
      "source": [
        "!autotrain llm --help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEf6G0iPc0Nr"
      },
      "source": [
        "## Step 5: Completed 🎉\n",
        "After the command above is completed your Model will be uploaded to Hugging Face.\n",
        "\n",
        "#### Learn more about AutoTrain (optional)\n",
        "If you want to learn more about what command-line flags are available"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIoxuAEAfJ4z"
      },
      "source": [
        "## Step 6: Inference Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYsYyXmrc0xu"
      },
      "outputs": [],
      "source": [
        "!autotrain llm -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m1ouhWhc2fr"
      },
      "outputs": [],
      "source": [
        "!pip install -q peft  accelerate bitsandbytes safetensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8s-nDnnPc--U"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import transformers\n",
        "adapters_name = \"ashishpatel26/mistral-7b-mj-finetuned\"\n",
        "model_name = \"bn22/Mistral-7B-Instruct-v0.1-sharded\" #\"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "\n",
        "device = \"cuda\" # the device to load the model onto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HosPywN_dEpl"
      },
      "outputs": [],
      "source": [
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "fcb820b4909e413e98603c195818e0d4",
            "6b0a6739adbe41e8a5c34f8a8868b977",
            "0b9514defba84991b4f36485b7e630fb",
            "6a7973751e4d4ca08ef4c53c97103868",
            "0deb9610aaff49c488b6e89139fe31df",
            "89f149a2080f4721a483ff535b6e6602",
            "4f62c475347944d6b18ce79d125386fc",
            "996db8f083904106913a3e4b4d6627c9",
            "f6b2ea40822a41899aae6768c5a34c73",
            "4523834103534e2b9fb804bdb5265a1e",
            "3912010e0694457f9f777c1bbb996967"
          ]
        },
        "id": "GtZx4CZUdt1f",
        "outputId": "c01df71d-a70e-48d3-d651-0061856f1b57"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fcb820b4909e413e98603c195818e0d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/11 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh5Xc0clfQkZ"
      },
      "source": [
        "## Step 7: Peft Model Loading with upload model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rt6sOPFVdvWX"
      },
      "outputs": [],
      "source": [
        "model = PeftModel.from_pretrained(model, adapters_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3OArVILeoZH",
        "outputId": "af68bc96-c9a8-4801-f8d6-5f2095101988"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully loaded the model bn22/Mistral-7B-Instruct-v0.1-sharded into memory\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.bos_token_id = 1\n",
        "\n",
        "stop_token_ids = [0]\n",
        "\n",
        "print(f\"Successfully loaded the model {model_name} into memory\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbOOX8cve0lR",
        "outputId": "3052b329-7bf5-4bb4-bec5-b71e881bbc21"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1539: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST] generate a midjourney prompt for A person walks in the rain [/INST] \"As you wander through the pouring rain, you can't help but wonder what the world would be like if things were different. What if the rain was a symbol of the turmoil in your life, and the sunshine promised a brighter future? What if you suddenly found yourself lost in a small town where time stood still, and the people were trapped in a time loop? As you struggle to find your way back to reality, you discover a mysterious stranger who seems to hold the key to unlocking the secrets of the town and your own past.\"</s>\n"
          ]
        }
      ],
      "source": [
        "text = \"[INST] generate a midjourney prompt for A person walks in the rain [/INST]\"\n",
        "\n",
        "encoded = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "model_input = encoded\n",
        "model.to(device)\n",
        "generated_ids = model.generate(**model_input, max_new_tokens=200, do_sample=True)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0208623ecbe842b6aa95c6fcee2b5ae8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb1457ce23234e2f86f20318977040c8",
            "placeholder": "​",
            "style": "IPY_MODEL_bd9ccb8d007e4659bce33beff90a9424",
            "value": "Connecting..."
          }
        },
        "02f6c09fcf8643389fc2cf1b3f9c668f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_d2c284204e554cbeae77e5f9e47248d7",
            "style": "IPY_MODEL_527e3d95ea90450086c9ff366c9bdc6f",
            "value": true
          }
        },
        "0b9514defba84991b4f36485b7e630fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_996db8f083904106913a3e4b4d6627c9",
            "max": 11,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6b2ea40822a41899aae6768c5a34c73",
            "value": 11
          }
        },
        "0cef031df2d64362bdfd505559345ce1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60ca62f461c744f9b5e8fb3993231e7c",
            "placeholder": "​",
            "style": "IPY_MODEL_8929504f7ade4c8bb2a0921e5b816b2d",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "0deb9610aaff49c488b6e89139fe31df": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "376ebd3722eb4145964fec20db32d5d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3912010e0694457f9f777c1bbb996967": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4523834103534e2b9fb804bdb5265a1e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47b948b31f0343ab9c2f5b9e4ec1fa3b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b7f2b4a2cee4d08a77f1e5b63be8692": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "4c4ecd6406da42aa8c7ee7e4f6c73f50": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "4f62c475347944d6b18ce79d125386fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "527e3d95ea90450086c9ff366c9bdc6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52a12ac6bd564e3cbc6f365151988631": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60ca62f461c744f9b5e8fb3993231e7c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a7973751e4d4ca08ef4c53c97103868": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4523834103534e2b9fb804bdb5265a1e",
            "placeholder": "​",
            "style": "IPY_MODEL_3912010e0694457f9f777c1bbb996967",
            "value": " 11/11 [01:22&lt;00:00,  5.56s/it]"
          }
        },
        "6b0a6739adbe41e8a5c34f8a8868b977": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89f149a2080f4721a483ff535b6e6602",
            "placeholder": "​",
            "style": "IPY_MODEL_4f62c475347944d6b18ce79d125386fc",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "74275c5ff18a4bb0912e482c8e535ea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "782834571ce74235b2c4948ff9a47b70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_e543526b1645407ba56f8d7d06a703ca",
            "style": "IPY_MODEL_4b7f2b4a2cee4d08a77f1e5b63be8692",
            "tooltip": ""
          }
        },
        "8929504f7ade4c8bb2a0921e5b816b2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89f149a2080f4721a483ff535b6e6602": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "996db8f083904106913a3e4b4d6627c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cd0a8a8677a43c480f2de842069968e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_52a12ac6bd564e3cbc6f365151988631",
            "placeholder": "​",
            "style": "IPY_MODEL_376ebd3722eb4145964fec20db32d5d3",
            "value": ""
          }
        },
        "b73f3add66a545efba5167a856ae5787": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47b948b31f0343ab9c2f5b9e4ec1fa3b",
            "placeholder": "​",
            "style": "IPY_MODEL_74275c5ff18a4bb0912e482c8e535ea7",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "bb1457ce23234e2f86f20318977040c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd9ccb8d007e4659bce33beff90a9424": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca3f33ecb9c8427ab5470edccde011ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_4c4ecd6406da42aa8c7ee7e4f6c73f50"
          }
        },
        "d2c284204e554cbeae77e5f9e47248d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e543526b1645407ba56f8d7d06a703ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6b2ea40822a41899aae6768c5a34c73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fcb820b4909e413e98603c195818e0d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6b0a6739adbe41e8a5c34f8a8868b977",
              "IPY_MODEL_0b9514defba84991b4f36485b7e630fb",
              "IPY_MODEL_6a7973751e4d4ca08ef4c53c97103868"
            ],
            "layout": "IPY_MODEL_0deb9610aaff49c488b6e89139fe31df"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
